{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36341,"status":"ok","timestamp":1718237549551,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"k0HxY-ApMAhX"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.4/953.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.6/780.6 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python-headless' candidate (version 4.8.0.74 at https://files.pythonhosted.org/packages/76/02/f128517f3ade4bb5f71e2afd8461dba70e3f466ce745fa1fd1fade9ad1b7/opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python-headless/) (requires-python:\u003e=3.6))\n","Reason for being yanked: deprecated, use 4.8.0.76\u001b[0m\u001b[33m\n","\u001b[0m  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","Collecting fastapi\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette\u003c0.38.0,\u003e=0.37.2 (from fastapi)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.3)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n","Collecting fastapi-cli\u003e=0.0.2 (from fastapi)\n","  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n","Collecting httpx\u003e=0.23.0 (from fastapi)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2\u003e=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n","Collecting python-multipart\u003e=0.0.7 (from fastapi)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,\u003e=4.0.1 (from fastapi)\n","  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson\u003e=3.2.1 (from fastapi)\n","  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator\u003e=2.0.0 (from fastapi)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Collecting uvicorn[standard]\u003e=0.12.0 (from fastapi)\n","  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dnspython\u003e=2.0.0 (from email_validator\u003e=2.0.0-\u003efastapi)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator\u003e=2.0.0-\u003efastapi) (2.10)\n","Requirement already satisfied: typer\u003e=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli\u003e=0.0.2-\u003efastapi) (0.12.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (2023.7.22)\n","Collecting httpcore==1.* (from httpx\u003e=0.23.0-\u003efastapi)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (1.3.1)\n","Collecting h11\u003c0.15,\u003e=0.13 (from httpcore==1.*-\u003ehttpx\u003e=0.23.0-\u003efastapi)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2\u003e=2.11.2-\u003efastapi) (2.1.5)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4-\u003efastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4-\u003efastapi) (2.18.4)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]\u003e=0.12.0-\u003efastapi) (8.1.7)\n","Collecting httptools\u003e=0.5.0 (from uvicorn[standard]\u003e=0.12.0-\u003efastapi)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dotenv\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]\u003e=0.12.0-\u003efastapi) (1.0.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]\u003e=0.12.0-\u003efastapi) (6.0.1)\n","Collecting uvloop!=0.15.0,!=0.15.1,\u003e=0.14.0 (from uvicorn[standard]\u003e=0.12.0-\u003efastapi)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles\u003e=0.13 (from uvicorn[standard]\u003e=0.12.0-\u003efastapi)\n","  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets\u003e=10.4 (from uvicorn[standard]\u003e=0.12.0-\u003efastapi)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio-\u003ehttpx\u003e=0.23.0-\u003efastapi) (1.2.1)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (13.7.1)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (0.1.2)\n","Installing collected packages: websockets, uvloop, ujson, python-multipart, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, fastapi-cli, fastapi\n","Successfully installed dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.5 python-multipart-0.0.9 starlette-0.37.2 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Requirement already satisfied: h11\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: typing-extensions\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n","Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n","Requirement already satisfied: Werkzeug\u003e=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n","Requirement already satisfied: Jinja2\u003e=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n","Requirement already satisfied: itsdangerous\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n","Requirement already satisfied: click\u003e=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2\u003e=3.0-\u003eflask) (2.1.5)\n","Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.32)\n","Requirement already satisfied: matplotlib\u003e=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python\u003e=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow\u003e=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml\u003e=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests\u003e=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy\u003e=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch\u003e=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n","Requirement already satisfied: torchvision\u003e=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n","Requirement already satisfied: tqdm\u003e=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas\u003e=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn\u003e=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: ultralytics-thop\u003e=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.2.8)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (1.2.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (0.10.0)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (4.53.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (1.4.5)\n","Requirement already satisfied: numpy\u003e=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (1.25.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (24.1)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.1.4-\u003eultralytics) (2023.4)\n","Requirement already satisfied: tzdata\u003e=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas\u003e=1.1.4-\u003eultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (2.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.14.0)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch\u003e=1.8.0-\u003eultralytics) (12.5.40)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cycler\u003e=0.10-\u003ematplotlib\u003e=3.3.0-\u003eultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.8.0-\u003eultralytics) (2.1.5)\n","Requirement already satisfied: mpmath\u003c1.4.0,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.8.0-\u003eultralytics) (1.3.0)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.8.0.74)\n","Requirement already satisfied: numpy\u003e=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.9)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch) (12.5.40)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (2.1.5)\n","Requirement already satisfied: mpmath\u003c1.4.0,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch) (1.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (2.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libjq1 libonig5\n","The following NEW packages will be installed:\n","  jq libjq1 libonig5\n","0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 357 kB of archives.\n","After this operation, 1,087 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libonig5 amd64 6.9.7.1-2build1 [172 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjq1 amd64 1.6-2.1ubuntu3 [133 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 jq amd64 1.6-2.1ubuntu3 [52.5 kB]\n","Fetched 357 kB in 2s (154 kB/s)\n","Selecting previously unselected package libonig5:amd64.\n","(Reading database ... 121913 files and directories currently installed.)\n","Preparing to unpack .../libonig5_6.9.7.1-2build1_amd64.deb ...\n","Unpacking libonig5:amd64 (6.9.7.1-2build1) ...\n","Selecting previously unselected package libjq1:amd64.\n","Preparing to unpack .../libjq1_1.6-2.1ubuntu3_amd64.deb ...\n","Unpacking libjq1:amd64 (1.6-2.1ubuntu3) ...\n","Selecting previously unselected package jq.\n","Preparing to unpack .../jq_1.6-2.1ubuntu3_amd64.deb ...\n","Unpacking jq (1.6-2.1ubuntu3) ...\n","Setting up libonig5:amd64 (6.9.7.1-2build1) ...\n","Setting up libjq1:amd64 (1.6-2.1ubuntu3) ...\n","Setting up jq (1.6-2.1ubuntu3) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyngrok\n","  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n","Requirement already satisfied: PyYAML\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.1.6\n","Collecting flask_ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n","Requirement already satisfied: Werkzeug\u003e=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask\u003e=0.8-\u003eflask_ngrok) (3.0.3)\n","Requirement already satisfied: Jinja2\u003e=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask\u003e=0.8-\u003eflask_ngrok) (3.1.4)\n","Requirement already satisfied: itsdangerous\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask\u003e=0.8-\u003eflask_ngrok) (2.2.0)\n","Requirement already satisfied: click\u003e=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask\u003e=0.8-\u003eflask_ngrok) (8.1.7)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eflask_ngrok) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eflask_ngrok) (2.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eflask_ngrok) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003eflask_ngrok) (2023.7.22)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2\u003e=3.0-\u003eFlask\u003e=0.8-\u003eflask_ngrok) (2.1.5)\n","Installing collected packages: flask_ngrok\n","Successfully installed flask_ngrok-0.0.25\n"]}],"source":["!pip install yolov8 -q\n","!pip install tqdm\n","!pip install fastapi\n","!pip install uvicorn\n","!pip install flask\n","!pip install ultralytics\n","!pip install opencv-python-headless\n","!pip install python-multipart\n","!pip install torch torchvision torchaudio\n","!pip install requests\n","!apt-get install jq\n","!pip install pyngrok\n","!pip install flask_ngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxQSNuJrehRK"},"outputs":[],"source":["#!sudo rm -f /usr/local/bin/ngrok\n","#!sudo rm -f /usr/bin/ngrok"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1643,"status":"ok","timestamp":1718237551181,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"4EElDTwrRCNG"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-14 04:53:30--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 54.161.241.46, 18.205.222.128, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9019411 (8.6M) [application/octet-stream]\n","Saving to: ‘ngrok.zip’\n","\n","ngrok.zip           100%[===================\u003e]   8.60M  4.71MB/s    in 1.8s    \n","\n","2024-06-14 04:53:33 (4.71 MB/s) - ‘ngrok.zip’ saved [9019411/9019411]\n","\n","Archive:  ngrok.zip\n","  inflating: ngrok                   \n"]}],"source":["!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip -O ngrok.zip\n","!unzip ngrok.zip\n","!sudo mv ngrok /usr/local/bin/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718237551183,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"1J9CaL75eomK"},"outputs":[{"name":"stdout","output_type":"stream","text":["ngrok version 3.11.0\n"]}],"source":["!ngrok version"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5457,"status":"ok","timestamp":1718237556638,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"dF-IfhCVPHVf"},"outputs":[{"name":"stdout","output_type":"stream","text":[""]}],"source":["from flask import Flask, request, send_file\n","from flask_ngrok import run_with_ngrok\n","from threading import Thread\n","import subprocess\n","import cv2\n","import os\n","import csv\n","from datetime import datetime\n","from collections import defaultdict\n","from copy import deepcopy\n","from ultralytics import YOLO\n","import requests\n","import time\n","from pyngrok import ngrok\n","\n","# ngrok 인증 토큰 설정\n","authtoken = \"2hVJXCKSqQraJv65bFACCo5I75J_4LXEAMxm8kNxYGAVgJUQV\"\n","ngrok.set_auth_token(authtoken)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":808,"status":"ok","timestamp":1718237557444,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"XtoWKB55QV2i"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://romantic-goshawk-comic.ngrok-free.app\n"]}],"source":["# ngrok을 통해 포트 5000을 연결\n","public_url = ngrok.connect(5000,hostname='romantic-goshawk-comic.ngrok-free.app').public_url  # Use a different port to avoid conflict\n","print(public_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579},"executionInfo":{"elapsed":1479,"status":"error","timestamp":1717999664718,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"QWFUqWkEgh1Q","outputId":"8e7cc21c-e8a8-4799-d518-c788271e028a"},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for LSTMAutoencoder:\n\tMissing key(s) in state_dict: \"encoder.weight_ih_l1\", \"encoder.weight_hh_l1\", \"encoder.bias_ih_l1\", \"encoder.bias_hh_l1\", \"decoder.weight_ih_l1\", \"decoder.weight_hh_l1\", \"decoder.bias_ih_l1\", \"decoder.bias_hh_l1\". \n\tUnexpected key(s) in state_dict: \"encoder2.weight_ih_l0\", \"encoder2.weight_hh_l0\", \"encoder2.bias_ih_l0\", \"encoder2.bias_hh_l0\", \"decoder2.weight_ih_l0\", \"decoder2.weight_hh_l0\", \"decoder2.bias_ih_l0\", \"decoder2.bias_hh_l0\". \n\tsize mismatch for encoder.weight_ih_l0: copying a param with shape torch.Size([400, 38]) from checkpoint, the shape in current model is torch.Size([512, 38]).\n\tsize mismatch for encoder.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for encoder.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.weight_ih_l0: copying a param with shape torch.Size([400, 50]) from checkpoint, the shape in current model is torch.Size([152, 128]).\n\tsize mismatch for decoder.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([152, 38]).\n\tsize mismatch for decoder.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([152]).\n\tsize mismatch for decoder.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([152]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-14-630d5c5a8325\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 61\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/taba/준형/LSTM_20240610_005340_best.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 61\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTMAutoencoder:\n\tMissing key(s) in state_dict: \"encoder.weight_ih_l1\", \"encoder.weight_hh_l1\", \"encoder.bias_ih_l1\", \"encoder.bias_hh_l1\", \"decoder.weight_ih_l1\", \"decoder.weight_hh_l1\", \"decoder.bias_ih_l1\", \"decoder.bias_hh_l1\". \n\tUnexpected key(s) in state_dict: \"encoder2.weight_ih_l0\", \"encoder2.weight_hh_l0\", \"encoder2.bias_ih_l0\", \"encoder2.bias_hh_l0\", \"decoder2.weight_ih_l0\", \"decoder2.weight_hh_l0\", \"decoder2.bias_ih_l0\", \"decoder2.bias_hh_l0\". \n\tsize mismatch for encoder.weight_ih_l0: copying a param with shape torch.Size([400, 38]) from checkpoint, the shape in current model is torch.Size([512, 38]).\n\tsize mismatch for encoder.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for encoder.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.weight_ih_l0: copying a param with shape torch.Size([400, 50]) from checkpoint, the shape in current model is torch.Size([152, 128]).\n\tsize mismatch for decoder.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([152, 38]).\n\tsize mismatch for decoder.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([152]).\n\tsize mismatch for decoder.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([152])."]}],"source":["from flask import Flask, request, send_file\n","from flask_ngrok import run_with_ngrok\n","import cv2\n","import csv\n","from datetime import datetime\n","from collections import defaultdict\n","from copy import deepcopy\n","from ultralytics import YOLO\n","import logging\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)  # 이는 flask_ngrok에서 사용하는 것입니다.\n","\n","header = [\n","    \"Filename\", \"Frame\", \"ID\", \"X\", \"Y\", \"Width\", \"Height\",\n","    \"Keypoint_0\", \"Keypoint_1\", \"Keypoint_2\", \"Keypoint_3\", \"Keypoint_4\", \"Keypoint_5\",\n","    \"Keypoint_6\", \"Keypoint_7\", \"Keypoint_8\", \"Keypoint_9\", \"Keypoint_10\", \"Keypoint_11\",\n","    \"Keypoint_12\", \"Keypoint_13\", \"Keypoint_14\", \"Keypoint_15\", \"Keypoint_16\", \"Keypoint_17\",\n","    \"Keypoint_18\", \"Keypoint_19\", \"Keypoint_20\", \"Keypoint_21\", \"Keypoint_22\", \"Keypoint_23\",\n","    \"Keypoint_24\", \"Keypoint_25\", \"Keypoint_26\", \"Keypoint_27\", \"Keypoint_28\", \"Keypoint_29\",\n","    \"Keypoint_30\", \"Keypoint_31\", \"Keypoint_32\", \"Keypoint_33\",\n","]\n","\n","model = YOLO(\"yolov8n-pose.pt\")\n","standard_width = 320\n","standard_height = 240\n","\n","@app.route('/process-video/', methods=['POST'])\n","def process_video():\n","    logger.info(\"start\")\n","    file = request.files['file']\n","    file_path = \"uploaded_video.mp4\"\n","    file.save(file_path)\n","\n","    output_file_path = \"output.csv\"\n","    cap = cv2.VideoCapture(file_path)\n","    frame_count = 0\n","    id_count = 0\n","    track_history = defaultdict(lambda: [])\n","\n","    logger.info(\"create csv...........\")\n","    with open(output_file_path, \"w\") as c_file:\n","        writer = csv.writer(c_file, delimiter=\",\")\n","        writer.writerow(header)\n","\n","        logger.info(\"feature extract\")\n","        while cap.isOpened():\n","            success, frame = cap.read()\n","            if not success:\n","                break\n","            frame_count += 1\n","            frame = cv2.resize(frame, (standard_width, standard_height))\n","            results = model.track(frame, persist=True, verbose=False)\n","            if results[0].boxes and results[0].boxes.id:\n","                track_ids = results[0].boxes.id.int().cpu().tolist()\n","                for i, box in zip(range(len(track_ids)), results[0].boxes.xywhn.cpu()):\n","                    keypoints = results[0].keypoints.xyn[i].cpu().numpy().flatten().tolist()\n","                    box_list = box.numpy().flatten().tolist()\n","                    box_and_keypoints = box_list + keypoints\n","                    track_history[track_ids[i]].append([[frame_count], deepcopy(box_and_keypoints)])\n","\n","        for key in track_history.keys():\n","            for f_count, b_and_k in track_history[key]:\n","                row = [\"uploaded_video.mp4\"] + f_count + [id_count + key] + b_and_k\n","                writer.writerow(row)\n","\n","        id_count += len(track_history.keys())\n","        cap.release()\n","\n","    return send_file(output_file_path, mimetype='text/csv', as_attachment=True, download_name=\"output.csv\")\n","\n","if __name__ == '__main__':\n","    app.run()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbraCYmjSVT9","outputId":"2932352c-16d8-4311-afdf-12865f1a9c82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.51M/6.51M [00:00\u003c00:00, 81.9MB/s]\n"]},{"name":"stdout","output_type":"stream","text":[" * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":[" * Running on http://romantic-goshawk-comic.ngrok-free.app\n"," * Traffic stats available on http://127.0.0.1:4040\n"]}],"source":["# ngrok을 통해 포트 5000을 연결\n","public_url = ngrok.connect(5000,hostname='romantic-goshawk-comic.ngrok-free.app').public_url  # Use a different port to avoid conflict\n","\n","from flask import Flask, request, jsonify\n","from flask_ngrok import run_with_ngrok\n","import cv2\n","import csv\n","import torch\n","import torch.nn as nn\n","import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","from collections import defaultdict\n","from copy import deepcopy\n","from ultralytics import YOLO\n","import logging\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)  # 이는 flask_ngrok에서 사용하는 것입니다.\n","\n","header = [\n","    \"Filename\", \"Frame\", \"ID\", \"X\", \"Y\", \"Width\", \"Height\",\n","    \"Keypoint_0\", \"Keypoint_1\", \"Keypoint_2\", \"Keypoint_3\", \"Keypoint_4\", \"Keypoint_5\",\n","    \"Keypoint_6\", \"Keypoint_7\", \"Keypoint_8\", \"Keypoint_9\", \"Keypoint_10\", \"Keypoint_11\",\n","    \"Keypoint_12\", \"Keypoint_13\", \"Keypoint_14\", \"Keypoint_15\", \"Keypoint_16\", \"Keypoint_17\",\n","    \"Keypoint_18\", \"Keypoint_19\", \"Keypoint_20\", \"Keypoint_21\", \"Keypoint_22\", \"Keypoint_23\",\n","    \"Keypoint_24\", \"Keypoint_25\", \"Keypoint_26\", \"Keypoint_27\", \"Keypoint_28\", \"Keypoint_29\",\n","    \"Keypoint_30\", \"Keypoint_31\", \"Keypoint_32\", \"Keypoint_33\",\n","]\n","\n","model = YOLO(\"yolov8n-pose.pt\")\n","standard_width = 320\n","standard_height = 240\n","\n","# 학습된 모델 클래스 정의\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self, sequence_length, n_features, prediction_time):\n","        super(LSTMAutoencoder, self).__init__()\n","        self.encoder = nn.LSTM(\n","            input_size=n_features, hidden_size=100, num_layers=1, batch_first=True\n","        )\n","        self.encoder2 = nn.LSTM(\n","            input_size=100, hidden_size=50, num_layers=1, batch_first=True\n","        )\n","        self.decoder = nn.LSTM(\n","            input_size=50, hidden_size=100, num_layers=1, batch_first=True\n","        )\n","        self.decoder2 = nn.LSTM(\n","            input_size=100, hidden_size=n_features, num_layers=1, batch_first=True\n","        )\n","\n","    def forward(self, x):\n","        x, _ = self.encoder(x)\n","        x, _ = self.encoder2(x)\n","        x, _ = self.decoder(x)\n","        x, _ = self.decoder2(x)\n","        return x\n","\n","# 모델 로드\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","sequence_length = 20  # 모델 학습 시 사용된 시퀀스 길이\n","n_features = 38  # 모델 학습 시 사용된 특징 수\n","prediction_time = 1  # 모델 학습 시 사용된 예측 시간\n","\n","trained_model = LSTMAutoencoder(sequence_length, n_features, prediction_time)\n","checkpoint = torch.load(\"/content/drive/MyDrive/taba/준형/LSTM_20240610_005340_best.pth\", map_location=device)\n","\n","# # 체크포인트의 키와 값을 출력 (디버깅 목적)\n","# for key, value in checkpoint.items():\n","#     print(f\"{key}: {type(value)}\")\n","\n","# # 필요시 세부 정보도 출력\n","# for key, value in checkpoint.items():\n","#     if isinstance(value, dict):\n","#         print(f\"{key}:\")\n","#         for subkey, subvalue in value.items():\n","#             print(f\"  {subkey}: {type(subvalue)}\")\n","#     elif isinstance(value, torch.Tensor):\n","#         print(f\"{key}: Tensor of shape {value.shape}\")\n","#     else:\n","#         print(f\"{key}: {value}\")\n","\n","trained_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","trained_model.to(device)\n","trained_model.eval()\n","\n","# 임계값 설정 (이상치 탐지를 위한)\n","thr = 0.3\n","\n","def detect_anomalies(data, sequence_length):\n","    # 데이터 전처리 및 모델 예측\n","    data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n","    with torch.no_grad():\n","        predictions = trained_model(data_tensor)\n","    # 이상치 탐지 로직\n","    loss_fn = nn.MSELoss(reduction=\"none\")\n","    loss = loss_fn(predictions, data_tensor)\n","    anomalies = loss.mean(dim=2) \u003e thr  # threshold 값을 넘으면 이상치로 간주\n","\n","    # 이상치가 발생한 시퀀스의 시작 프레임 인덱스 반환\n","    anomaly_indices = np.where(anomalies.cpu().numpy())[0]  # 이상치가 발생한 시퀀스 인덱스\n","    anomaly_frames = anomaly_indices * sequence_length  # 시퀀스 인덱스를 프레임 인덱스로 변환\n","\n","    # 중복된 프레임 번호 제거 및 이상치가 연속된 경우 단일 시작 프레임만 반환\n","    unique_anomaly_frames = sorted(set(anomaly_frames))\n","    unique_anomaly_frames = [int(frame) for frame in unique_anomaly_frames]  # int64를 int로 변환\n","    return unique_anomaly_frames\n","\n","\n","\n","\n","\n","@app.route('/process-video/', methods=['POST'])\n","def process_video():\n","    logger.info(\"start\")\n","    file = request.files['file']\n","    file_path = \"uploaded_video.mp4\"\n","    file.save(file_path)\n","\n","    output_file_path = \"output.csv\"\n","    cap = cv2.VideoCapture(file_path)\n","    frame_count = 0\n","    id_count = 0\n","    track_history = defaultdict(lambda: [])\n","\n","    logger.info(\"create csv...........\")\n","    with open(output_file_path, \"w\") as c_file:\n","        writer = csv.writer(c_file, delimiter=\",\")\n","        writer.writerow(header)\n","\n","        logger.info(\"feature extract\")\n","        while cap.isOpened():\n","            success, frame = cap.read()\n","            if not success:\n","                break\n","            frame_count += 1\n","            frame = cv2.resize(frame, (standard_width, standard_height))\n","            results = model.track(frame, persist=True, verbose=False)\n","            if results[0].boxes and results[0].boxes.id:\n","                track_ids = results[0].boxes.id.int().cpu().tolist()\n","                for i, box in zip(range(len(track_ids)), results[0].boxes.xywhn.cpu()):\n","                    keypoints = results[0].keypoints.xyn[i].cpu().numpy().flatten().tolist()\n","                    box_list = box.numpy().flatten().tolist()\n","                    box_and_keypoints = box_list + keypoints\n","                    track_history[track_ids[i]].append([[frame_count], deepcopy(box_and_keypoints)])\n","\n","        for key in track_history.keys():\n","            for f_count, b_and_k in track_history[key]:\n","                row = [\"uploaded_video.mp4\"] + f_count + [id_count + key] + b_and_k\n","                writer.writerow(row)\n","\n","        id_count += len(track_history.keys())\n","        cap.release()\n","\n","    # CSV 파일 로드\n","    df = pd.read_csv(output_file_path)\n","\n","    # 필요한 데이터 추출 및 전처리\n","    data = df.iloc[:, 8:].values  # Keypoint 데이터 추출\n","\n","    # 데이터 크기 확인\n","    total_data_points = data.shape[0]\n","    logger.info(f\"Total data points: {total_data_points}\")\n","\n","    # 패딩을 사용하여 데이터 크기를 맞춤\n","    if total_data_points % (sequence_length * n_features) != 0:\n","        pad_length = (sequence_length * n_features) - (total_data_points % (sequence_length * n_features))\n","        data = np.pad(data, ((0, pad_length), (0, 0)), mode='constant')\n","        logger.info(f\"Padded data points: {data.shape[0]}\")\n","\n","    data = data.reshape(-1, sequence_length, n_features)\n","\n","    # 이상행동 탐지\n","    anomaly_frames = detect_anomalies(data, sequence_length)\n","\n","    # 결과 반환\n","    return jsonify({\"anomaly_frames\": anomaly_frames})\n","\n","if __name__ == '__main__':\n","    app.run()\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1472,"status":"ok","timestamp":1718191366542,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"e1BdTS92Idxz"},"outputs":[],"source":["from collections import defaultdict\n","import cv2\n","import numpy as np\n","import torch as torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.preprocessing import MinMaxScaler\n","from ultralytics import YOLO\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2278,"status":"ok","timestamp":1718191369118,"user":{"displayName":"신유원","userId":"18136435380045022017"},"user_tz":-540},"id":"OIa19s4dRcRA"},"outputs":[],"source":["from torchvision import transforms\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cNB5Uw8CLEnX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.51M/6.51M [00:00\u003c00:00, 144MB/s]\n"]},{"name":"stdout","output_type":"stream","text":[" * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":[" * Running on http://romantic-goshawk-comic.ngrok-free.app\n"," * Traffic stats available on http://127.0.0.1:4040\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lapx\u003e=0.5.2'] not found, attempting AutoUpdate...\n","Collecting lapx\u003e=0.5.2\n","  Downloading lapx-0.5.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 12.5 MB/s eta 0:00:00\n","Requirement already satisfied: numpy\u003e=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lapx\u003e=0.5.2) (1.25.2)\n","Installing collected packages: lapx\n","Successfully installed lapx-0.5.9\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 8.6s, installed 1 package: ['lapx\u003e=0.5.2']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","0: 480x640 1 person, 411.8ms\n","Speed: 17.9ms preprocess, 411.8ms inference, 33.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.2ms\n","Speed: 2.5ms preprocess, 202.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.0ms\n","Speed: 2.4ms preprocess, 209.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.7ms\n","Speed: 5.1ms preprocess, 197.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 279.6ms\n","Speed: 2.3ms preprocess, 279.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 316.7ms\n","Speed: 2.4ms preprocess, 316.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 318.7ms\n","Speed: 2.4ms preprocess, 318.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 326.8ms\n","Speed: 2.3ms preprocess, 326.8ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 349.7ms\n","Speed: 2.2ms preprocess, 349.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 320.9ms\n","Speed: 3.2ms preprocess, 320.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 337.3ms\n","Speed: 3.8ms preprocess, 337.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 327.1ms\n","Speed: 5.9ms preprocess, 327.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 317.8ms\n","Speed: 2.3ms preprocess, 317.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 327.9ms\n","Speed: 3.8ms preprocess, 327.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 328.2ms\n","Speed: 3.4ms preprocess, 328.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 348.4ms\n","Speed: 2.5ms preprocess, 348.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 358.6ms\n","Speed: 5.5ms preprocess, 358.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 349.5ms\n","Speed: 2.3ms preprocess, 349.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 327.9ms\n","Speed: 2.5ms preprocess, 327.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.4ms\n","Speed: 2.3ms preprocess, 215.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 227.6ms\n","Speed: 2.4ms preprocess, 227.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 214.7ms\n","Speed: 2.4ms preprocess, 214.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 229.5ms\n","Speed: 2.3ms preprocess, 229.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.1ms\n","Speed: 2.3ms preprocess, 212.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 216.9ms\n","Speed: 2.7ms preprocess, 216.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.0ms\n","Speed: 2.7ms preprocess, 208.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 227.3ms\n","Speed: 2.8ms preprocess, 227.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.7ms\n","Speed: 3.8ms preprocess, 215.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 220.4ms\n","Speed: 3.7ms preprocess, 220.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 218.5ms\n","Speed: 3.5ms preprocess, 218.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 233.0ms\n","Speed: 2.4ms preprocess, 233.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 229.0ms\n","Speed: 2.5ms preprocess, 229.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 216.6ms\n","Speed: 2.3ms preprocess, 216.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 226.2ms\n","Speed: 2.3ms preprocess, 226.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 214.2ms\n","Speed: 3.4ms preprocess, 214.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 224.0ms\n","Speed: 2.8ms preprocess, 224.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.7ms\n","Speed: 2.4ms preprocess, 211.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 227.3ms\n","Speed: 3.7ms preprocess, 227.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 223.2ms\n","Speed: 4.0ms preprocess, 223.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 217.5ms\n","Speed: 3.9ms preprocess, 217.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 219.7ms\n","Speed: 2.3ms preprocess, 219.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 216.8ms\n","Speed: 2.4ms preprocess, 216.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 228.4ms\n","Speed: 4.3ms preprocess, 228.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.9ms\n","Speed: 2.3ms preprocess, 208.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 230.7ms\n","Speed: 2.8ms preprocess, 230.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.7ms\n","Speed: 2.6ms preprocess, 221.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.9ms\n","Speed: 3.2ms preprocess, 215.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 213.4ms\n","Speed: 2.3ms preprocess, 213.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.2ms\n","Speed: 2.7ms preprocess, 221.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 222.5ms\n","Speed: 2.4ms preprocess, 222.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.8ms\n","Speed: 2.6ms preprocess, 211.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.1ms\n","Speed: 2.8ms preprocess, 212.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 225.2ms\n","Speed: 2.6ms preprocess, 225.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.9ms\n","Speed: 2.4ms preprocess, 215.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 279.3ms\n","Speed: 2.7ms preprocess, 279.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 363.0ms\n","Speed: 2.2ms preprocess, 363.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 322.2ms\n","Speed: 2.4ms preprocess, 322.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 357.6ms\n","Speed: 2.3ms preprocess, 357.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 331.6ms\n","Speed: 2.3ms preprocess, 331.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 315.0ms\n","Speed: 2.2ms preprocess, 315.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 348.9ms\n","Speed: 2.2ms preprocess, 348.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 309.4ms\n","Speed: 2.4ms preprocess, 309.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 324.8ms\n","Speed: 2.2ms preprocess, 324.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 315.8ms\n","Speed: 2.4ms preprocess, 315.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 346.7ms\n","Speed: 3.6ms preprocess, 346.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 374.9ms\n","Speed: 3.8ms preprocess, 374.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 351.1ms\n","Speed: 3.3ms preprocess, 351.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 315.0ms\n","Speed: 2.3ms preprocess, 315.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 325.0ms\n","Speed: 4.0ms preprocess, 325.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 254.1ms\n","Speed: 2.6ms preprocess, 254.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.8ms\n","Speed: 2.6ms preprocess, 201.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.8ms\n","Speed: 2.3ms preprocess, 206.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.2ms\n","Speed: 2.7ms preprocess, 195.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.0ms\n","Speed: 2.9ms preprocess, 194.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.9ms\n","Speed: 2.7ms preprocess, 209.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.5ms\n","Speed: 3.5ms preprocess, 213.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.8ms\n","Speed: 4.6ms preprocess, 197.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.0ms\n","Speed: 2.9ms preprocess, 194.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.0ms\n","Speed: 2.3ms preprocess, 201.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.0ms\n","Speed: 2.4ms preprocess, 210.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.3ms\n","Speed: 2.5ms preprocess, 202.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.8ms\n","Speed: 2.5ms preprocess, 194.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.5ms\n","Speed: 2.7ms preprocess, 204.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.2ms\n","Speed: 2.5ms preprocess, 209.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.2ms\n","Speed: 2.6ms preprocess, 198.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.0ms\n","Speed: 2.6ms preprocess, 192.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.9ms\n","Speed: 3.8ms preprocess, 208.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 204.7ms\n","Speed: 2.9ms preprocess, 204.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.6ms\n","Speed: 2.4ms preprocess, 199.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 192.8ms\n","Speed: 2.4ms preprocess, 192.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.5ms\n","Speed: 4.0ms preprocess, 195.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 204.6ms\n","Speed: 2.3ms preprocess, 204.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.4ms\n","Speed: 3.8ms preprocess, 198.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 189.1ms\n","Speed: 3.6ms preprocess, 189.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.3ms\n","Speed: 2.4ms preprocess, 197.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.9ms\n","Speed: 2.2ms preprocess, 208.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.9ms\n","Speed: 2.4ms preprocess, 207.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.1ms\n","Speed: 2.3ms preprocess, 207.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.2ms\n","Speed: 2.8ms preprocess, 208.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.6ms\n","Speed: 2.8ms preprocess, 206.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.4ms\n","Speed: 2.4ms preprocess, 199.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.2ms\n","Speed: 2.9ms preprocess, 203.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 193.4ms\n","Speed: 3.2ms preprocess, 193.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.1ms\n","Speed: 2.9ms preprocess, 205.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.3ms\n","Speed: 2.4ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.6ms\n","Speed: 2.9ms preprocess, 205.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 194.7ms\n","Speed: 2.8ms preprocess, 194.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.2ms\n","Speed: 2.6ms preprocess, 201.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 214.2ms\n","Speed: 8.5ms preprocess, 214.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 296.2ms\n","Speed: 2.3ms preprocess, 296.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 304.5ms\n","Speed: 2.4ms preprocess, 304.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 295.6ms\n","Speed: 5.5ms preprocess, 295.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 299.3ms\n","Speed: 2.3ms preprocess, 299.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 315.3ms\n","Speed: 2.5ms preprocess, 315.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 289.4ms\n","Speed: 2.3ms preprocess, 289.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 297.6ms\n","Speed: 2.5ms preprocess, 297.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.0ms\n","Speed: 2.3ms preprocess, 316.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 299.2ms\n","Speed: 2.2ms preprocess, 299.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 307.0ms\n","Speed: 2.2ms preprocess, 307.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 317.9ms\n","Speed: 3.7ms preprocess, 317.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 316.8ms\n","Speed: 2.3ms preprocess, 316.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 332.6ms\n","Speed: 2.3ms preprocess, 332.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 296.0ms\n","Speed: 4.1ms preprocess, 296.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 301.1ms\n","Speed: 4.8ms preprocess, 301.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 249.9ms\n","Speed: 7.1ms preprocess, 249.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.4ms\n","Speed: 2.9ms preprocess, 192.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.6ms\n","Speed: 2.4ms preprocess, 191.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.6ms\n","Speed: 2.5ms preprocess, 197.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.3ms\n","Speed: 2.5ms preprocess, 196.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.1ms\n","Speed: 2.4ms preprocess, 191.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.0ms\n","Speed: 2.5ms preprocess, 194.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.3ms\n","Speed: 2.4ms preprocess, 194.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.7ms\n","Speed: 2.4ms preprocess, 193.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.6ms\n","Speed: 2.3ms preprocess, 213.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.9ms\n","Speed: 2.6ms preprocess, 195.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.6ms\n","Speed: 2.4ms preprocess, 195.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.0ms\n","Speed: 2.6ms preprocess, 199.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.8ms\n","Speed: 2.5ms preprocess, 215.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 193.6ms\n","Speed: 2.6ms preprocess, 193.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.3ms\n","Speed: 3.8ms preprocess, 194.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.9ms\n","Speed: 2.7ms preprocess, 196.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.0ms\n","Speed: 4.0ms preprocess, 208.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 190.0ms\n","Speed: 3.1ms preprocess, 190.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.8ms\n","Speed: 2.7ms preprocess, 191.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 189.8ms\n","Speed: 2.4ms preprocess, 189.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.2ms\n","Speed: 2.4ms preprocess, 199.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.1ms\n","Speed: 2.4ms preprocess, 194.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 189.8ms\n","Speed: 3.2ms preprocess, 189.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.7ms\n","Speed: 2.4ms preprocess, 196.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.3ms\n","Speed: 3.2ms preprocess, 200.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.6ms\n","Speed: 3.2ms preprocess, 204.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.4ms\n","Speed: 3.2ms preprocess, 192.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.9ms\n","Speed: 2.3ms preprocess, 199.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.3ms\n","Speed: 2.2ms preprocess, 199.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.7ms\n","Speed: 2.4ms preprocess, 208.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.2ms\n","Speed: 7.0ms preprocess, 193.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.3ms\n","Speed: 2.3ms preprocess, 191.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.5ms\n","Speed: 2.6ms preprocess, 194.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 215.8ms\n","Speed: 2.3ms preprocess, 215.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.0ms\n","Speed: 2.3ms preprocess, 193.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.0ms\n","Speed: 2.5ms preprocess, 195.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.0ms\n","Speed: 4.0ms preprocess, 192.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.7ms\n","Speed: 3.1ms preprocess, 193.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.4ms\n","Speed: 2.5ms preprocess, 207.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 190.8ms\n","Speed: 2.6ms preprocess, 190.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.4ms\n","Speed: 2.3ms preprocess, 194.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 295.6ms\n","Speed: 2.4ms preprocess, 295.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 310.9ms\n","Speed: 4.4ms preprocess, 310.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 291.8ms\n","Speed: 6.4ms preprocess, 291.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 290.4ms\n","Speed: 2.4ms preprocess, 290.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.0ms\n","Speed: 2.3ms preprocess, 319.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 296.1ms\n","Speed: 2.4ms preprocess, 296.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 290.3ms\n","Speed: 2.3ms preprocess, 290.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 311.5ms\n","Speed: 2.3ms preprocess, 311.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 286.8ms\n","Speed: 2.3ms preprocess, 286.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 300.6ms\n","Speed: 2.4ms preprocess, 300.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 316.9ms\n","Speed: 3.4ms preprocess, 316.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 300.5ms\n","Speed: 2.4ms preprocess, 300.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 313.4ms\n","Speed: 5.9ms preprocess, 313.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 321.6ms\n","Speed: 2.5ms preprocess, 321.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [14/Jun/2024 05:19:55] \"POST /process-video/ HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["\n","0: 480x640 1 person, 250.3ms\n","Speed: 4.4ms preprocess, 250.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 227.8ms\n","Speed: 2.4ms preprocess, 227.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.7ms\n","Speed: 2.7ms preprocess, 209.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.3ms\n","Speed: 2.4ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.4ms\n","Speed: 3.3ms preprocess, 201.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 237.6ms\n","Speed: 3.5ms preprocess, 237.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.6ms\n","Speed: 2.8ms preprocess, 213.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 206.5ms\n","Speed: 4.6ms preprocess, 206.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 224.2ms\n","Speed: 2.4ms preprocess, 224.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.1ms\n","Speed: 2.7ms preprocess, 221.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.6ms\n","Speed: 2.9ms preprocess, 212.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.6ms\n","Speed: 2.6ms preprocess, 207.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 209.5ms\n","Speed: 3.5ms preprocess, 209.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 216.0ms\n","Speed: 2.4ms preprocess, 216.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.2ms\n","Speed: 4.0ms preprocess, 205.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 200.8ms\n","Speed: 3.4ms preprocess, 200.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.2ms\n","Speed: 3.1ms preprocess, 213.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 226.6ms\n","Speed: 2.4ms preprocess, 226.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.4ms\n","Speed: 2.7ms preprocess, 208.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 304.8ms\n","Speed: 2.6ms preprocess, 304.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 327.4ms\n","Speed: 2.5ms preprocess, 327.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 301.8ms\n","Speed: 2.3ms preprocess, 301.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 315.5ms\n","Speed: 2.3ms preprocess, 315.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 323.4ms\n","Speed: 2.4ms preprocess, 323.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 308.7ms\n","Speed: 2.6ms preprocess, 308.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.5ms\n","Speed: 2.4ms preprocess, 316.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 346.6ms\n","Speed: 3.0ms preprocess, 346.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 320.3ms\n","Speed: 2.4ms preprocess, 320.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 302.7ms\n","Speed: 2.5ms preprocess, 302.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 328.0ms\n","Speed: 2.2ms preprocess, 328.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 326.5ms\n","Speed: 2.2ms preprocess, 326.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 351.5ms\n","Speed: 2.4ms preprocess, 351.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 332.9ms\n","Speed: 2.2ms preprocess, 332.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 325.0ms\n","Speed: 2.4ms preprocess, 325.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 300.4ms\n","Speed: 2.6ms preprocess, 300.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.0ms\n","Speed: 2.6ms preprocess, 221.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.4ms\n","Speed: 2.4ms preprocess, 206.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.9ms\n","Speed: 2.3ms preprocess, 203.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.5ms\n","Speed: 2.3ms preprocess, 221.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.7ms\n","Speed: 2.3ms preprocess, 212.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.9ms\n","Speed: 2.3ms preprocess, 205.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.9ms\n","Speed: 2.3ms preprocess, 215.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 236.4ms\n","Speed: 2.4ms preprocess, 236.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.1ms\n","Speed: 3.9ms preprocess, 212.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.0ms\n","Speed: 2.2ms preprocess, 207.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.3ms\n","Speed: 2.4ms preprocess, 203.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.0ms\n","Speed: 4.2ms preprocess, 211.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.3ms\n","Speed: 2.8ms preprocess, 208.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.5ms\n","Speed: 2.4ms preprocess, 203.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 226.3ms\n","Speed: 2.7ms preprocess, 226.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.8ms\n","Speed: 2.9ms preprocess, 206.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 200.5ms\n","Speed: 2.8ms preprocess, 200.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 204.1ms\n","Speed: 4.0ms preprocess, 204.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 228.0ms\n","Speed: 2.4ms preprocess, 228.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.7ms\n","Speed: 4.6ms preprocess, 206.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 202.9ms\n","Speed: 3.2ms preprocess, 202.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.3ms\n","Speed: 3.1ms preprocess, 211.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.7ms\n","Speed: 3.0ms preprocess, 221.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.5ms\n","Speed: 3.3ms preprocess, 202.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.1ms\n","Speed: 2.7ms preprocess, 207.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.5ms\n","Speed: 2.3ms preprocess, 205.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.5ms\n","Speed: 3.0ms preprocess, 207.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.0ms\n","Speed: 6.3ms preprocess, 195.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.0ms\n","Speed: 2.3ms preprocess, 197.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.1ms\n","Speed: 2.5ms preprocess, 203.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 216.3ms\n","Speed: 3.4ms preprocess, 216.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.9ms\n","Speed: 3.1ms preprocess, 198.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.6ms\n","Speed: 3.3ms preprocess, 205.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 211.0ms\n","Speed: 2.3ms preprocess, 211.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 215.6ms\n","Speed: 2.5ms preprocess, 215.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.4ms\n","Speed: 2.6ms preprocess, 199.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.9ms\n","Speed: 2.3ms preprocess, 198.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 322.6ms\n","Speed: 2.5ms preprocess, 322.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 306.6ms\n","Speed: 5.8ms preprocess, 306.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 308.3ms\n","Speed: 2.7ms preprocess, 308.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 318.5ms\n","Speed: 2.3ms preprocess, 318.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 313.4ms\n","Speed: 5.6ms preprocess, 313.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 312.2ms\n","Speed: 2.3ms preprocess, 312.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 322.2ms\n","Speed: 5.4ms preprocess, 322.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 303.1ms\n","Speed: 4.6ms preprocess, 303.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 324.9ms\n","Speed: 4.1ms preprocess, 324.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 322.6ms\n","Speed: 2.7ms preprocess, 322.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 325.2ms\n","Speed: 2.8ms preprocess, 325.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 356.4ms\n","Speed: 2.3ms preprocess, 356.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 323.4ms\n","Speed: 2.3ms preprocess, 323.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 331.0ms\n","Speed: 2.5ms preprocess, 331.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 312.3ms\n","Speed: 2.4ms preprocess, 312.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.7ms\n","Speed: 2.4ms preprocess, 212.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.8ms\n","Speed: 2.5ms preprocess, 198.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 224.9ms\n","Speed: 2.7ms preprocess, 224.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 222.8ms\n","Speed: 2.6ms preprocess, 222.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 202.3ms\n","Speed: 3.3ms preprocess, 202.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 200.8ms\n","Speed: 2.3ms preprocess, 200.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.1ms\n","Speed: 2.3ms preprocess, 195.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 210.4ms\n","Speed: 2.6ms preprocess, 210.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.8ms\n","Speed: 2.3ms preprocess, 197.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.4ms\n","Speed: 2.3ms preprocess, 197.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 191.1ms\n","Speed: 2.4ms preprocess, 191.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.9ms\n","Speed: 3.0ms preprocess, 207.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 192.7ms\n","Speed: 2.6ms preprocess, 192.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.8ms\n","Speed: 2.6ms preprocess, 196.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.8ms\n","Speed: 4.5ms preprocess, 197.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.6ms\n","Speed: 2.9ms preprocess, 199.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.1ms\n","Speed: 4.4ms preprocess, 197.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.1ms\n","Speed: 2.7ms preprocess, 197.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.5ms\n","Speed: 3.9ms preprocess, 205.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.6ms\n","Speed: 2.4ms preprocess, 196.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 193.2ms\n","Speed: 3.5ms preprocess, 193.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 192.3ms\n","Speed: 2.7ms preprocess, 192.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.2ms\n","Speed: 3.2ms preprocess, 207.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.4ms\n","Speed: 3.3ms preprocess, 199.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 189.5ms\n","Speed: 2.3ms preprocess, 189.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.7ms\n","Speed: 2.4ms preprocess, 198.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.9ms\n","Speed: 2.3ms preprocess, 215.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.1ms\n","Speed: 2.3ms preprocess, 196.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 192.0ms\n","Speed: 2.3ms preprocess, 192.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.5ms\n","Speed: 5.6ms preprocess, 205.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 218.2ms\n","Speed: 3.2ms preprocess, 218.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.9ms\n","Speed: 3.1ms preprocess, 199.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.9ms\n","Speed: 3.6ms preprocess, 196.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.3ms\n","Speed: 2.3ms preprocess, 195.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 216.0ms\n","Speed: 3.0ms preprocess, 216.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.7ms\n","Speed: 2.9ms preprocess, 202.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.6ms\n","Speed: 2.7ms preprocess, 192.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.3ms\n","Speed: 2.8ms preprocess, 197.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 257.7ms\n","Speed: 3.8ms preprocess, 257.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 309.4ms\n","Speed: 2.3ms preprocess, 309.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 297.3ms\n","Speed: 2.3ms preprocess, 297.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 310.2ms\n","Speed: 2.3ms preprocess, 310.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 285.4ms\n","Speed: 2.9ms preprocess, 285.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 315.4ms\n","Speed: 2.4ms preprocess, 315.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 305.5ms\n","Speed: 2.4ms preprocess, 305.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 310.3ms\n","Speed: 2.5ms preprocess, 310.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 309.1ms\n","Speed: 2.8ms preprocess, 309.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 288.8ms\n","Speed: 2.2ms preprocess, 288.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 302.5ms\n","Speed: 2.7ms preprocess, 302.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 324.4ms\n","Speed: 2.3ms preprocess, 324.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.5ms\n","Speed: 4.1ms preprocess, 316.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 328.2ms\n","Speed: 2.4ms preprocess, 328.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 331.0ms\n","Speed: 2.3ms preprocess, 331.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 314.2ms\n","Speed: 2.3ms preprocess, 314.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 279.3ms\n","Speed: 2.3ms preprocess, 279.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.9ms\n","Speed: 2.4ms preprocess, 209.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.3ms\n","Speed: 2.3ms preprocess, 194.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.8ms\n","Speed: 2.5ms preprocess, 195.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.2ms\n","Speed: 7.0ms preprocess, 205.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 211.1ms\n","Speed: 3.3ms preprocess, 211.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.7ms\n","Speed: 2.3ms preprocess, 196.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.0ms\n","Speed: 2.3ms preprocess, 193.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.7ms\n","Speed: 2.5ms preprocess, 208.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.0ms\n","Speed: 2.4ms preprocess, 204.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.5ms\n","Speed: 3.2ms preprocess, 197.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.8ms\n","Speed: 3.5ms preprocess, 202.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.1ms\n","Speed: 2.8ms preprocess, 205.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.2ms\n","Speed: 2.6ms preprocess, 201.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.7ms\n","Speed: 2.9ms preprocess, 205.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.0ms\n","Speed: 2.4ms preprocess, 201.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 212.0ms\n","Speed: 3.4ms preprocess, 212.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.9ms\n","Speed: 3.5ms preprocess, 202.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.1ms\n","Speed: 5.3ms preprocess, 207.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.0ms\n","Speed: 4.6ms preprocess, 206.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.7ms\n","Speed: 2.6ms preprocess, 213.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.8ms\n","Speed: 2.4ms preprocess, 193.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.9ms\n","Speed: 2.4ms preprocess, 213.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.6ms\n","Speed: 2.4ms preprocess, 197.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.2ms\n","Speed: 2.2ms preprocess, 206.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 212.1ms\n","Speed: 2.3ms preprocess, 212.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 218.1ms\n","Speed: 2.6ms preprocess, 218.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.9ms\n","Speed: 2.4ms preprocess, 195.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.3ms\n","Speed: 2.2ms preprocess, 210.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.1ms\n","Speed: 2.3ms preprocess, 201.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.0ms\n","Speed: 2.4ms preprocess, 207.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.7ms\n","Speed: 2.4ms preprocess, 193.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.5ms\n","Speed: 2.6ms preprocess, 206.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.5ms\n","Speed: 2.8ms preprocess, 202.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.3ms\n","Speed: 3.4ms preprocess, 196.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 216.9ms\n","Speed: 2.7ms preprocess, 216.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.0ms\n","Speed: 2.3ms preprocess, 203.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 190.3ms\n","Speed: 2.3ms preprocess, 190.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.3ms\n","Speed: 2.3ms preprocess, 192.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [14/Jun/2024 05:23:19] \"POST /process-video/ HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["\n","0: 480x640 1 person, 264.6ms\n","Speed: 2.5ms preprocess, 264.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 211.6ms\n","Speed: 2.8ms preprocess, 211.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.6ms\n","Speed: 2.4ms preprocess, 207.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.5ms\n","Speed: 2.3ms preprocess, 210.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.3ms\n","Speed: 3.1ms preprocess, 196.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.9ms\n","Speed: 2.3ms preprocess, 194.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.8ms\n","Speed: 2.5ms preprocess, 210.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 215.1ms\n","Speed: 2.3ms preprocess, 215.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.2ms\n","Speed: 2.3ms preprocess, 206.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.3ms\n","Speed: 4.8ms preprocess, 203.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 209.7ms\n","Speed: 2.4ms preprocess, 209.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 218.3ms\n","Speed: 2.3ms preprocess, 218.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.8ms\n","Speed: 3.1ms preprocess, 201.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 194.4ms\n","Speed: 2.9ms preprocess, 194.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.6ms\n","Speed: 2.7ms preprocess, 208.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.7ms\n","Speed: 2.6ms preprocess, 203.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.0ms\n","Speed: 2.6ms preprocess, 197.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.9ms\n","Speed: 2.8ms preprocess, 199.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.2ms\n","Speed: 4.4ms preprocess, 211.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 214.3ms\n","Speed: 2.8ms preprocess, 214.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.1ms\n","Speed: 2.4ms preprocess, 197.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 193.7ms\n","Speed: 2.3ms preprocess, 193.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.6ms\n","Speed: 2.4ms preprocess, 201.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 213.1ms\n","Speed: 4.9ms preprocess, 213.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 193.2ms\n","Speed: 2.3ms preprocess, 193.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.1ms\n","Speed: 3.2ms preprocess, 196.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 258.5ms\n","Speed: 3.5ms preprocess, 258.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 312.9ms\n","Speed: 2.4ms preprocess, 312.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 303.3ms\n","Speed: 2.2ms preprocess, 303.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 307.0ms\n","Speed: 2.6ms preprocess, 307.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 312.9ms\n","Speed: 2.4ms preprocess, 312.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 309.3ms\n","Speed: 2.3ms preprocess, 309.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 323.0ms\n","Speed: 2.3ms preprocess, 323.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 302.2ms\n","Speed: 2.4ms preprocess, 302.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 304.9ms\n","Speed: 2.4ms preprocess, 304.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 311.3ms\n","Speed: 2.3ms preprocess, 311.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 331.5ms\n","Speed: 2.3ms preprocess, 331.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 320.3ms\n","Speed: 2.3ms preprocess, 320.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 354.5ms\n","Speed: 2.2ms preprocess, 354.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 315.9ms\n","Speed: 2.3ms preprocess, 315.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 333.5ms\n","Speed: 2.4ms preprocess, 333.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 342.8ms\n","Speed: 2.6ms preprocess, 342.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 227.6ms\n","Speed: 2.3ms preprocess, 227.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 202.7ms\n","Speed: 2.4ms preprocess, 202.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.7ms\n","Speed: 3.0ms preprocess, 207.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.2ms\n","Speed: 2.4ms preprocess, 197.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.1ms\n","Speed: 3.1ms preprocess, 198.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.7ms\n","Speed: 2.4ms preprocess, 212.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.8ms\n","Speed: 2.3ms preprocess, 203.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.9ms\n","Speed: 2.5ms preprocess, 201.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.2ms\n","Speed: 2.3ms preprocess, 199.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.4ms\n","Speed: 2.3ms preprocess, 208.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 210.8ms\n","Speed: 2.5ms preprocess, 210.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.7ms\n","Speed: 5.0ms preprocess, 195.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 202.5ms\n","Speed: 2.3ms preprocess, 202.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.2ms\n","Speed: 2.4ms preprocess, 215.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.0ms\n","Speed: 2.4ms preprocess, 203.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.6ms\n","Speed: 2.3ms preprocess, 201.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.7ms\n","Speed: 2.6ms preprocess, 200.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 221.7ms\n","Speed: 2.3ms preprocess, 221.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.2ms\n","Speed: 2.3ms preprocess, 194.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.5ms\n","Speed: 3.4ms preprocess, 200.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.5ms\n","Speed: 3.4ms preprocess, 206.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 217.4ms\n","Speed: 2.3ms preprocess, 217.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.4ms\n","Speed: 3.2ms preprocess, 198.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.3ms\n","Speed: 3.3ms preprocess, 206.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.3ms\n","Speed: 3.5ms preprocess, 202.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 221.9ms\n","Speed: 2.5ms preprocess, 221.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.6ms\n","Speed: 3.2ms preprocess, 201.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.8ms\n","Speed: 2.2ms preprocess, 208.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.2ms\n","Speed: 2.2ms preprocess, 203.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 226.8ms\n","Speed: 2.3ms preprocess, 226.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.0ms\n","Speed: 3.5ms preprocess, 198.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.6ms\n","Speed: 3.2ms preprocess, 203.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.2ms\n","Speed: 2.9ms preprocess, 201.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 212.3ms\n","Speed: 2.3ms preprocess, 212.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.0ms\n","Speed: 2.4ms preprocess, 198.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.1ms\n","Speed: 2.2ms preprocess, 203.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.7ms\n","Speed: 2.6ms preprocess, 207.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 211.7ms\n","Speed: 2.3ms preprocess, 211.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 288.4ms\n","Speed: 3.4ms preprocess, 288.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 307.3ms\n","Speed: 3.1ms preprocess, 307.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 361.5ms\n","Speed: 3.8ms preprocess, 361.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 365.8ms\n","Speed: 3.8ms preprocess, 365.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 400.3ms\n","Speed: 3.1ms preprocess, 400.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 348.3ms\n","Speed: 5.5ms preprocess, 348.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 364.8ms\n","Speed: 2.4ms preprocess, 364.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 319.0ms\n","Speed: 3.1ms preprocess, 319.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 310.9ms\n","Speed: 4.2ms preprocess, 310.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 363.9ms\n","Speed: 2.8ms preprocess, 363.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 314.6ms\n","Speed: 2.8ms preprocess, 314.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 349.8ms\n","Speed: 2.3ms preprocess, 349.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 345.2ms\n","Speed: 3.1ms preprocess, 345.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 321.7ms\n","Speed: 3.0ms preprocess, 321.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 331.6ms\n","Speed: 3.4ms preprocess, 331.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 259.9ms\n","Speed: 2.3ms preprocess, 259.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.8ms\n","Speed: 2.3ms preprocess, 195.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.6ms\n","Speed: 3.3ms preprocess, 215.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.3ms\n","Speed: 2.5ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.7ms\n","Speed: 3.6ms preprocess, 195.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.2ms\n","Speed: 2.2ms preprocess, 196.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.1ms\n","Speed: 2.3ms preprocess, 215.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 199.3ms\n","Speed: 2.6ms preprocess, 199.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.7ms\n","Speed: 2.6ms preprocess, 197.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.5ms\n","Speed: 2.5ms preprocess, 205.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 216.7ms\n","Speed: 2.6ms preprocess, 216.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 216.8ms\n","Speed: 3.0ms preprocess, 216.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.0ms\n","Speed: 3.2ms preprocess, 208.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 200.1ms\n","Speed: 2.3ms preprocess, 200.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.4ms\n","Speed: 3.2ms preprocess, 215.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.4ms\n","Speed: 2.5ms preprocess, 208.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.2ms\n","Speed: 2.2ms preprocess, 195.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.9ms\n","Speed: 2.2ms preprocess, 197.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.6ms\n","Speed: 3.2ms preprocess, 207.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 210.5ms\n","Speed: 3.7ms preprocess, 210.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 194.0ms\n","Speed: 3.6ms preprocess, 194.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.6ms\n","Speed: 3.2ms preprocess, 196.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.8ms\n","Speed: 2.2ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.4ms\n","Speed: 5.7ms preprocess, 195.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.9ms\n","Speed: 2.5ms preprocess, 198.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.9ms\n","Speed: 2.3ms preprocess, 196.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.0ms\n","Speed: 2.5ms preprocess, 206.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.3ms\n","Speed: 2.3ms preprocess, 197.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.1ms\n","Speed: 2.3ms preprocess, 197.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.8ms\n","Speed: 2.3ms preprocess, 199.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 215.7ms\n","Speed: 2.3ms preprocess, 215.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.9ms\n","Speed: 3.6ms preprocess, 198.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.2ms\n","Speed: 2.3ms preprocess, 197.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.1ms\n","Speed: 2.2ms preprocess, 198.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.9ms\n","Speed: 2.4ms preprocess, 208.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.5ms\n","Speed: 2.3ms preprocess, 197.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.9ms\n","Speed: 2.3ms preprocess, 193.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.1ms\n","Speed: 3.5ms preprocess, 198.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 215.6ms\n","Speed: 2.9ms preprocess, 215.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 298.7ms\n","Speed: 3.0ms preprocess, 298.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.6ms\n","Speed: 4.6ms preprocess, 316.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 312.2ms\n","Speed: 2.8ms preprocess, 312.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.3ms\n","Speed: 2.3ms preprocess, 316.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 309.6ms\n","Speed: 2.7ms preprocess, 309.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 321.4ms\n","Speed: 2.4ms preprocess, 321.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 304.3ms\n","Speed: 2.2ms preprocess, 304.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 307.6ms\n","Speed: 2.4ms preprocess, 307.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 314.8ms\n","Speed: 2.2ms preprocess, 314.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 325.2ms\n","Speed: 2.4ms preprocess, 325.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 317.8ms\n","Speed: 4.9ms preprocess, 317.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 311.4ms\n","Speed: 2.3ms preprocess, 311.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 331.9ms\n","Speed: 2.3ms preprocess, 331.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 323.9ms\n","Speed: 10.1ms preprocess, 323.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 318.5ms\n","Speed: 9.1ms preprocess, 318.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 340.9ms\n","Speed: 2.5ms preprocess, 340.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.4ms\n","Speed: 4.0ms preprocess, 191.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.9ms\n","Speed: 2.2ms preprocess, 192.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.8ms\n","Speed: 2.3ms preprocess, 207.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.4ms\n","Speed: 2.2ms preprocess, 196.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 211.2ms\n","Speed: 2.3ms preprocess, 211.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.1ms\n","Speed: 2.6ms preprocess, 200.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 213.3ms\n","Speed: 3.0ms preprocess, 213.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.9ms\n","Speed: 2.7ms preprocess, 196.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.7ms\n","Speed: 2.6ms preprocess, 208.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.9ms\n","Speed: 3.0ms preprocess, 192.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.8ms\n","Speed: 3.2ms preprocess, 206.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.3ms\n","Speed: 3.2ms preprocess, 209.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 212.6ms\n","Speed: 3.0ms preprocess, 212.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.5ms\n","Speed: 2.5ms preprocess, 196.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 221.8ms\n","Speed: 3.3ms preprocess, 221.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.6ms\n","Speed: 3.3ms preprocess, 205.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.7ms\n","Speed: 2.3ms preprocess, 205.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.2ms\n","Speed: 2.3ms preprocess, 210.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.0ms\n","Speed: 2.5ms preprocess, 209.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.8ms\n","Speed: 2.8ms preprocess, 193.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.7ms\n","Speed: 2.3ms preprocess, 206.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.7ms\n","Speed: 3.2ms preprocess, 191.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.8ms\n","Speed: 2.6ms preprocess, 200.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.4ms\n","Speed: 2.2ms preprocess, 198.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 187.9ms\n","Speed: 2.2ms preprocess, 187.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.3ms\n","Speed: 2.3ms preprocess, 194.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.9ms\n","Speed: 2.5ms preprocess, 196.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.4ms\n","Speed: 2.8ms preprocess, 200.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 192.8ms\n","Speed: 2.9ms preprocess, 192.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.6ms\n","Speed: 4.2ms preprocess, 204.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [14/Jun/2024 05:26:04] \"POST /process-video/ HTTP/1.1\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["\n","0: 480x640 1 person, 322.2ms\n","Speed: 3.6ms preprocess, 322.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.3ms\n","Speed: 2.5ms preprocess, 209.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.7ms\n","Speed: 2.3ms preprocess, 209.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.8ms\n","Speed: 2.0ms preprocess, 209.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 219.6ms\n","Speed: 5.3ms preprocess, 219.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.9ms\n","Speed: 2.1ms preprocess, 199.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 208.1ms\n","Speed: 2.2ms preprocess, 208.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 199.6ms\n","Speed: 2.2ms preprocess, 199.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.8ms\n","Speed: 2.5ms preprocess, 212.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 203.6ms\n","Speed: 2.1ms preprocess, 203.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.5ms\n","Speed: 2.3ms preprocess, 201.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 209.7ms\n","Speed: 2.1ms preprocess, 209.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.4ms\n","Speed: 2.3ms preprocess, 208.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 3 persons, 201.3ms\n","Speed: 2.1ms preprocess, 201.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.8ms\n","Speed: 2.1ms preprocess, 199.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 210.4ms\n","Speed: 2.1ms preprocess, 210.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 214.3ms\n","Speed: 3.3ms preprocess, 214.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 204.0ms\n","Speed: 4.1ms preprocess, 204.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.1ms\n","Speed: 2.8ms preprocess, 206.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 204.1ms\n","Speed: 2.2ms preprocess, 204.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.8ms\n","Speed: 2.2ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 191.1ms\n","Speed: 2.3ms preprocess, 191.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 191.6ms\n","Speed: 2.2ms preprocess, 191.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.9ms\n","Speed: 2.3ms preprocess, 206.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.4ms\n","Speed: 2.5ms preprocess, 208.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.5ms\n","Speed: 2.3ms preprocess, 198.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.1ms\n","Speed: 2.2ms preprocess, 196.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 202.2ms\n","Speed: 2.9ms preprocess, 202.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.6ms\n","Speed: 2.2ms preprocess, 205.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 207.5ms\n","Speed: 2.0ms preprocess, 207.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 208.8ms\n","Speed: 2.1ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 214.3ms\n","Speed: 2.0ms preprocess, 214.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 209.2ms\n","Speed: 2.3ms preprocess, 209.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.7ms\n","Speed: 2.2ms preprocess, 206.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 206.0ms\n","Speed: 2.2ms preprocess, 206.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 209.8ms\n","Speed: 6.7ms preprocess, 209.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 205.6ms\n","Speed: 2.2ms preprocess, 205.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 211.4ms\n","Speed: 3.1ms preprocess, 211.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 212.8ms\n","Speed: 2.3ms preprocess, 212.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 240.4ms\n","Speed: 2.4ms preprocess, 240.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 343.1ms\n","Speed: 2.3ms preprocess, 343.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 285.9ms\n","Speed: 2.1ms preprocess, 285.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 297.8ms\n","Speed: 2.2ms preprocess, 297.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 307.0ms\n","Speed: 2.2ms preprocess, 307.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 306.6ms\n","Speed: 3.3ms preprocess, 306.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 321.9ms\n","Speed: 2.6ms preprocess, 321.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 324.6ms\n","Speed: 2.3ms preprocess, 324.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 324.0ms\n","Speed: 5.0ms preprocess, 324.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 324.6ms\n","Speed: 2.8ms preprocess, 324.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.1ms\n","Speed: 7.1ms preprocess, 316.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 316.7ms\n","Speed: 2.1ms preprocess, 316.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 372.7ms\n","Speed: 7.9ms preprocess, 372.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 321.1ms\n","Speed: 2.2ms preprocess, 321.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 334.3ms\n","Speed: 2.4ms preprocess, 334.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 301.1ms\n","Speed: 3.0ms preprocess, 301.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 210.7ms\n","Speed: 3.3ms preprocess, 210.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 200.8ms\n","Speed: 2.1ms preprocess, 200.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.8ms\n","Speed: 2.1ms preprocess, 201.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 186.8ms\n","Speed: 2.2ms preprocess, 186.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.5ms\n","Speed: 2.2ms preprocess, 201.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 191.6ms\n","Speed: 2.2ms preprocess, 191.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 224.0ms\n","Speed: 2.3ms preprocess, 224.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.6ms\n","Speed: 2.2ms preprocess, 200.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.5ms\n","Speed: 2.4ms preprocess, 193.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.5ms\n","Speed: 2.1ms preprocess, 206.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 229.4ms\n","Speed: 2.2ms preprocess, 229.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.1ms\n","Speed: 2.2ms preprocess, 198.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 207.1ms\n","Speed: 2.1ms preprocess, 207.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.7ms\n","Speed: 2.1ms preprocess, 209.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 210.8ms\n","Speed: 5.7ms preprocess, 210.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.8ms\n","Speed: 2.2ms preprocess, 200.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.3ms\n","Speed: 2.2ms preprocess, 205.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.9ms\n","Speed: 2.1ms preprocess, 193.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.5ms\n","Speed: 2.1ms preprocess, 204.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.8ms\n","Speed: 2.2ms preprocess, 199.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.4ms\n","Speed: 2.0ms preprocess, 194.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.3ms\n","Speed: 2.1ms preprocess, 200.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.3ms\n","Speed: 2.1ms preprocess, 209.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.1ms\n","Speed: 2.3ms preprocess, 199.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.5ms\n","Speed: 2.1ms preprocess, 202.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 216.8ms\n","Speed: 2.2ms preprocess, 216.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 214.0ms\n","Speed: 2.2ms preprocess, 214.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.9ms\n","Speed: 2.1ms preprocess, 197.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 225.8ms\n","Speed: 2.3ms preprocess, 225.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.5ms\n","Speed: 8.0ms preprocess, 319.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 316.1ms\n","Speed: 2.3ms preprocess, 316.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 309.7ms\n","Speed: 10.1ms preprocess, 309.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 343.4ms\n","Speed: 2.1ms preprocess, 343.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 297.3ms\n","Speed: 2.2ms preprocess, 297.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 307.4ms\n","Speed: 2.3ms preprocess, 307.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 481.2ms\n","Speed: 2.3ms preprocess, 481.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 451.1ms\n","Speed: 2.5ms preprocess, 451.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 328.8ms\n","Speed: 3.0ms preprocess, 328.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 347.9ms\n","Speed: 3.6ms preprocess, 347.9ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 419.8ms\n","Speed: 5.2ms preprocess, 419.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 497.0ms\n","Speed: 3.7ms preprocess, 497.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 469.8ms\n","Speed: 2.4ms preprocess, 469.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 486.8ms\n","Speed: 5.5ms preprocess, 486.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 428.4ms\n","Speed: 2.3ms preprocess, 428.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 552.8ms\n","Speed: 4.4ms preprocess, 552.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 672.5ms\n","Speed: 2.8ms preprocess, 672.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 520.1ms\n","Speed: 8.3ms preprocess, 520.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 525.0ms\n","Speed: 2.6ms preprocess, 525.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 622.8ms\n","Speed: 3.1ms preprocess, 622.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 500.1ms\n","Speed: 2.2ms preprocess, 500.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 409.1ms\n","Speed: 2.1ms preprocess, 409.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 338.2ms\n","Speed: 2.1ms preprocess, 338.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 370.7ms\n","Speed: 2.2ms preprocess, 370.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 342.0ms\n","Speed: 3.9ms preprocess, 342.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 309.4ms\n","Speed: 2.4ms preprocess, 309.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 318.7ms\n","Speed: 2.1ms preprocess, 318.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.9ms\n","Speed: 2.8ms preprocess, 195.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 194.3ms\n","Speed: 2.5ms preprocess, 194.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 215.3ms\n","Speed: 2.5ms preprocess, 215.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 201.4ms\n","Speed: 2.2ms preprocess, 201.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 194.9ms\n","Speed: 2.4ms preprocess, 194.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 197.6ms\n","Speed: 2.2ms preprocess, 197.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 218.3ms\n","Speed: 5.6ms preprocess, 218.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.5ms\n","Speed: 3.0ms preprocess, 203.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.5ms\n","Speed: 2.3ms preprocess, 200.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 194.8ms\n","Speed: 2.2ms preprocess, 194.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.3ms\n","Speed: 2.3ms preprocess, 205.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 198.8ms\n","Speed: 3.5ms preprocess, 198.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 204.4ms\n","Speed: 2.3ms preprocess, 204.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.7ms\n","Speed: 2.3ms preprocess, 196.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.0ms\n","Speed: 2.3ms preprocess, 199.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 212.0ms\n","Speed: 2.2ms preprocess, 212.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 193.0ms\n","Speed: 3.5ms preprocess, 193.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.0ms\n","Speed: 2.2ms preprocess, 202.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.4ms\n","Speed: 3.2ms preprocess, 200.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.2ms\n","Speed: 3.3ms preprocess, 206.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.6ms\n","Speed: 2.6ms preprocess, 209.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.1ms\n","Speed: 2.3ms preprocess, 196.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.5ms\n","Speed: 2.3ms preprocess, 205.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 215.2ms\n","Speed: 2.2ms preprocess, 215.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 196.9ms\n","Speed: 3.3ms preprocess, 196.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 198.4ms\n","Speed: 2.1ms preprocess, 198.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 195.4ms\n","Speed: 2.4ms preprocess, 195.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 persons, 221.8ms\n","Speed: 2.2ms preprocess, 221.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.9ms\n","Speed: 2.2ms preprocess, 202.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.6ms\n","Speed: 2.3ms preprocess, 202.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.4ms\n","Speed: 2.3ms preprocess, 205.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 227.8ms\n","Speed: 2.2ms preprocess, 227.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 203.6ms\n","Speed: 2.2ms preprocess, 203.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.9ms\n","Speed: 2.2ms preprocess, 200.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.1ms\n","Speed: 2.4ms preprocess, 197.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 229.7ms\n","Speed: 2.1ms preprocess, 229.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 197.6ms\n","Speed: 2.1ms preprocess, 197.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 190.8ms\n","Speed: 2.5ms preprocess, 190.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 195.5ms\n","Speed: 2.4ms preprocess, 195.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 251.2ms\n","Speed: 2.2ms preprocess, 251.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 315.9ms\n","Speed: 2.9ms preprocess, 315.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 307.8ms\n","Speed: 2.1ms preprocess, 307.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 346.8ms\n","Speed: 3.6ms preprocess, 346.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 296.2ms\n","Speed: 2.1ms preprocess, 296.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 333.9ms\n","Speed: 4.8ms preprocess, 333.9ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.0ms\n","Speed: 2.3ms preprocess, 319.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 310.1ms\n","Speed: 2.3ms preprocess, 310.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 318.0ms\n","Speed: 2.2ms preprocess, 318.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.5ms\n","Speed: 4.5ms preprocess, 319.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 310.0ms\n","Speed: 2.0ms preprocess, 310.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.7ms\n","Speed: 2.0ms preprocess, 319.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 308.4ms\n","Speed: 2.3ms preprocess, 308.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.2ms\n","Speed: 2.3ms preprocess, 319.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 332.0ms\n","Speed: 2.1ms preprocess, 332.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 319.6ms\n","Speed: 2.2ms preprocess, 319.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 326.4ms\n","Speed: 2.3ms preprocess, 326.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 206.3ms\n","Speed: 7.8ms preprocess, 206.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 196.7ms\n","Speed: 2.1ms preprocess, 196.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.2ms\n","Speed: 2.2ms preprocess, 202.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.2ms\n","Speed: 2.4ms preprocess, 201.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 200.6ms\n","Speed: 2.1ms preprocess, 200.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 199.5ms\n","Speed: 2.2ms preprocess, 199.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 201.8ms\n","Speed: 2.6ms preprocess, 201.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.6ms\n","Speed: 2.2ms preprocess, 205.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 202.3ms\n","Speed: 2.3ms preprocess, 202.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 221.5ms\n","Speed: 2.2ms preprocess, 221.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 205.2ms\n","Speed: 2.3ms preprocess, 205.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 209.8ms\n","Speed: 2.6ms preprocess, 209.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 person, 189.5ms\n","Speed: 2.5ms preprocess, 189.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [14/Jun/2024 05:28:52] \"POST /process-video/ HTTP/1.1\" 200 -\n"]}],"source":["# ngrok을 통해 포트 5000을 연결\n","public_url = ngrok.connect(5000,hostname='romantic-goshawk-comic.ngrok-free.app').public_url  # Use a different port to avoid conflict\n","\n","from flask import Flask, request, jsonify\n","import cv2\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from datetime import datetime\n","from collections import defaultdict\n","from copy import deepcopy\n","from ultralytics import YOLO\n","from sklearn.preprocessing import MinMaxScaler\n","from flask_ngrok import run_with_ngrok\n","import logging\n","import os\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)  # ngrok 사용\n","\n","# YOLOv8 모델 로드\n","model = YOLO(\"yolov8n-pose.pt\")\n","\n","# LSTM 오토인코더 클래스 정의\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self, sequence_length, n_features, prediction_time):\n","        super(LSTMAutoencoder, self).__init__()\n","        self.sequence_length = sequence_length\n","        self.n_features = n_features\n","        self.prediction_time = prediction_time\n","\n","        self.encoder = nn.LSTM(input_size=n_features, hidden_size=100, batch_first=True)\n","        self.encoder2 = nn.LSTM(input_size=100, hidden_size=50, batch_first=True)\n","        self.repeat_vector = nn.Sequential(\n","            nn.ReplicationPad1d(padding=(0, prediction_time - 1)),\n","            nn.ReplicationPad1d(padding=(0, 0))\n","        )\n","        self.decoder = nn.LSTM(input_size=50, hidden_size=100, batch_first=True)\n","        self.decoder2 = nn.LSTM(input_size=100, hidden_size=n_features, batch_first=True)\n","\n","    def forward(self, x):\n","        _, (x, _) = self.encoder(x)\n","        _, (x, _) = self.encoder2(x)\n","        x = self.repeat_vector(x)\n","        _, (x, _) = self.decoder(x)\n","        _, (x, _) = self.decoder2(x)\n","        return x\n","\n","# 모델 인스턴스화 및 체크포인트 로드\n","sequence_length = 20\n","prediction_time = 1\n","n_features = 38\n","threshold = 0.02\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","autoencoder_model = LSTMAutoencoder(sequence_length, n_features, prediction_time)\n","checkpoint = torch.load(\"/content/drive/MyDrive/taba/유원/model/LSTMAutoEncoder/LSTM_20240610_005340_best.pth\", map_location=device)\n","state_dict = checkpoint['model_state_dict']\n","autoencoder_model.load_state_dict(state_dict)\n","autoencoder_model.to(device)\n","autoencoder_model.eval()\n","\n","# 비디오 처리 및 이상 행동 탐지 함수\n","def process_video(file_path):\n","    cap = cv2.VideoCapture(file_path)\n","    standard_width = 640\n","    standard_height = 480\n","\n","    track_history = defaultdict(lambda: [])\n","    id_buffers = defaultdict(lambda: [])\n","\n","    # MSE 계산 함수\n","    def calculate_mse(seq1, seq2):\n","        return np.mean(np.power(seq1 - seq2, 2))\n","\n","    anomaly_times = []\n","    frame_count = 0\n","    net_mse = 0\n","    avg_mse = 0\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    anomaly_start_time = None\n","    output_video_writer = None\n","\n","    while cap.isOpened():\n","        success, frame = cap.read()\n","        frame_count += 1\n","\n","        if success:\n","            frame = cv2.resize(frame, (standard_width, standard_height))\n","            results = model.track(frame, persist=True)\n","\n","            if results[0].boxes is not None:\n","                boxes = results[0].boxes.xywh.cpu()\n","\n","                if results[0].boxes.id is not None:\n","                    track_ids = results[0].boxes.id.int().cpu().tolist()\n","                    anomaly_text = \"\"\n","                    for i, box in zip(range(0, len(track_ids)), results[0].boxes.xywhn.cpu()):\n","                        x, y, w, h = box\n","                        keypoints = results[0].keypoints.xyn[i].cpu().numpy().flatten().tolist()\n","\n","                        id_buffers[track_ids[i]].append([float(x), float(y), float(w), float(h)] + keypoints)\n","\n","                        if len(id_buffers[track_ids[i]]) \u003e= 20:\n","                            buffer_array = np.array(id_buffers[track_ids[i]])\n","                            scaler = MinMaxScaler()\n","                            buffer_scaled = scaler.fit_transform(buffer_array)\n","                            x_pred = buffer_scaled[-sequence_length:].reshape(1, sequence_length, n_features)\n","                            x_pred = autoencoder_model(torch.tensor(x_pred, dtype=torch.float32).to(device))\n","                            x_pred_original = scaler.inverse_transform(x_pred.cpu().detach().numpy().reshape(-1, n_features))\n","                            mse = calculate_mse(buffer_array[-prediction_time:], x_pred_original)\n","                            net_mse = mse + net_mse\n","                            avg_mse = net_mse / frame_count\n","\n","                            if mse \u003e 1.5 * avg_mse * 0.25 + 0.75 * threshold:\n","                                if anomaly_start_time is None:\n","                                    anomaly_start_time = frame_count / fps\n","                                    output_video_writer = cv2.VideoWriter(\n","                                        f'anomaly_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.mp4',\n","                                        cv2.VideoWriter_fourcc(*'mp4v'),\n","                                        fps,\n","                                        (standard_width, standard_height)\n","                                    )\n","                                anomaly_text = f\"Anomaly detected for ID {track_ids[i]}\"\n","                                logger.info(anomaly_text)\n","                                output_video_writer.write(frame)\n","\n","                            else:\n","                                if anomaly_start_time is not None:\n","                                    anomaly_end_time = frame_count / fps\n","                                    anomaly_times.append((anomaly_start_time, anomaly_end_time))\n","                                    anomaly_start_time = None\n","                                    output_video_writer.release()\n","                                    output_video_writer = None\n","\n","                            id_buffers[track_ids[i]].pop(0)\n","                else:\n","                    anomaly_text = \"\"\n","                    track_ids = []\n","\n","                for box, track_id in zip(boxes, track_ids):\n","                    x, y, w, h = box\n","                    track = track_history[track_id]\n","                    track.append((float(x), float(y)))\n","                    if len(track) \u003e 30:\n","                        track.pop(0)\n","\n","                    points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n","                    cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=2)\n","\n","        else:\n","            break\n","\n","    if anomaly_start_time is not None:\n","        anomaly_end_time = frame_count / fps\n","        anomaly_times.append((anomaly_start_time, anomaly_end_time))\n","        output_video_writer.release()\n","\n","    cap.release()\n","\n","    with open('anomaly_times.txt', 'w') as f:\n","        for start, end in anomaly_times:\n","            f.write(f\"Anomaly from {start:.2f} seconds to {end:.2f} seconds\\n\")\n","\n","    return anomaly_times\n","\n","@app.route('/process-video/', methods=['POST'])\n","def upload_video():\n","    file = request.files['file']\n","    file_path = \"uploaded_video.mp4\"\n","    file.save(file_path)\n","    anomaly_times = process_video(file_path)\n","    return jsonify({\"anomaly_times\": anomaly_times})\n","\n","if __name__ == '__main__':\n","    app.run()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNvKLO/4Dzjz3trtF8iKyVu","gpuType":"T4","mount_file_id":"1J40iQBbfKFVIV3sYs-LirTGzRpMuxQGy","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}