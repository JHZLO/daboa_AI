{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOehH4qVR6v9gCZ8rfPs6Si"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Encoder   \n","Decoder   \n","LSTMAutoEncoder"],"metadata":{"id":"dExOnqCzG8zT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTITHMcNGZtn"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\"\"\"\n","LSTM output\n","- N : number of batches\n","- L : sequence lengh\n","- Q : input dim\n","- K : number of layers\n","- D : LSTM feature dimension\n","\n","Y,(hn,cn) = LSTM(X)\n","\n","- X : [N x L x Q] - `N` input sequnce of length `L` with `Q` dim.\n","- Y : [N x L x D] - `N` output sequnce of length `L` with `D` feature dim.\n","- hn : [K x N x D] - `K` (per each layer) of `N` final hidden state with  `D` feature dim.\n","- cn : [K x N x D] - `K` (per each layer) of `N` final hidden state with  `D` cell dim.\n","\"\"\"\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    input: input_seq: (batch_size, seq_len, n_features) -> (1, 20, 38)\n","    output: hidden_cell -> (hn, cn)\n","        -> ((num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size))\n","    \"\"\"\n","\n","    def __init__(self, num_layers, hidden_size, n_features, device):\n","        super(Encoder, self).__init__()\n","\n","        self.input_size = n_features\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.device = device\n","\n","        self.lstm = nn.LSTM(\n","            input_size=n_features,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","        )\n","\n","    def initHidden(self, batch_size):\n","        \"\"\"\n","        intialize hn, cn\n","        \"\"\"\n","        self.hidden_cell = (\n","            torch.randn(\n","                (self.num_layers, batch_size, self.hidden_size), dtype=torch.float\n","            ).to(self.device),\n","            torch.randn(\n","                (self.num_layers, batch_size, self.hidden_size), dtype=torch.float\n","            ).to(self.device),\n","        )\n","\n","    def forward(self, input_seq):\n","        self.initHidden(input_seq.shape[0])\n","        _, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n","        return self.hidden_cell\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    input: (input_seq, hidden_cell)\n","        input_seq:\n","        hidden_cell: encoder 에서 넘어온 hidden_cell (hn, cn)\n","    output:\n","        decoder output: (batch_size, seq_len, n_features) -> (1, 1, 38)\n","        linear output: (batch_size, n_features) -> (1, 38)\n","    \"\"\"\n","\n","    def __init__(self, num_layers, hidden_size, n_features, device):\n","        super(Decoder, self).__init__()\n","\n","        self.input_size = n_features\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.device = device\n","\n","        self.lstm = nn.LSTM(\n","            input_size=n_features,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","        )\n","        self.linear = nn.Linear(in_features=hidden_size, out_features=n_features)\n","\n","    def forward(self, input_seq, hidden_cell):\n","        output, hidden_cell = self.lstm(input_seq, hidden_cell)\n","        output = self.linear(output)\n","        return output, hidden_cell\n","\n","\n","class LSTMAutoEncoder(nn.Module):\n","    \"\"\"\n","    output: input seq_len(20) 모두 복원\n","        reconstruction 순서는 입력의 반대.\n","    \"\"\"\n","\n","    def __init__(self, num_layers, hidden_size, n_features, device):\n","        super(LSTMAutoEncoder, self).__init__()\n","        self.device = device\n","        self.encoder = Encoder(num_layers, hidden_size, n_features, device)\n","        self.decoder = Decoder(num_layers, hidden_size, n_features, device)\n","\n","    def forward(self, input_seq):\n","        output = torch.zeros(size=input_seq.shape, dtype=torch.float)\n","        hidden_cell = self.encoder(input_seq)\n","        input_decoder = torch.zeros(\n","            (input_seq.shape[0], 1, input_seq.shape[2]), dtype=torch.float\n","        ).to(self.device)\n","        for i in range(input_seq.shape[1] - 1, -1, -1):\n","            output_decoder, hidden_cell = self.decoder(input_decoder, hidden_cell)\n","            input_decoder = output_decoder\n","            output[:, i, :] = output_decoder[:, 0, :]\n","\n","        return output.to(self.device)"]},{"cell_type":"code","source":["import json\n","import os\n","import os.path as osp\n","from collections import defaultdict as dd\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import Dataset"],"metadata":{"id":"KbxGynXJJDL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVHsNrMwILoX","executionInfo":{"status":"ok","timestamp":1716915347365,"user_tz":-540,"elapsed":22389,"user":{"displayName":"신유원","userId":"18136435380045022017"}},"outputId":"6fcc9942-5e39-42a6-96b3-e15f92d4180f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["class AbnormalDataset(Dataset):\n","    def __init__(\n","        self,\n","        sequence_length=20,\n","        root=\"/content/drive/MyDrive/taba/유원/npy_from_MAEv2/절도 영상 원천 데이터\",\n","        label_root=\"/data/ephemeral/home/level2-3-cv-finalproject-cv-06/app/models/lstmae/dataset/label\",\n","    ):\n","        super().__init__()\n","        self.sequence_length = sequence_length\n","\n","        self.scaler = MinMaxScaler()\n","        # 데이터 값 [0,1] 범위로 scaling할때 사용\n","\n","        # Load the dataset\n","        file_list = os.listdir(root)\n","\n","        df_list = []\n","        self.length = 0\n","        self.range_table = []\n","        self.real_length = 0\n","        self.real_idx_table = []\n","\n","        for i, file_name in enumerate(file_list):\n","            dat = pd.read_csv(root + \"/\" + file_name)\n","            # dat.drop(columns=[\"Frame\"], inplace=True)  # Remove the 'Frame' column\n","\n","            print(f\"==>>{i}번째 dat.shape: {dat.shape}\")\n","\n","            id_counter = pd.Series(dat[\"ID\"]).value_counts(sort=False)\n","\n","            for id_to_del in id_counter[id_counter < sequence_length].index:\n","                dat.drop(dat[dat[\"ID\"] == id_to_del].index, inplace=True)\n","\n","            id_counter = pd.Series(dat[\"ID\"]).value_counts(sort=False)\n","\n","            print(f\"==>>{i}번째 처리 후 dat.shape: {dat.shape}\")\n","            assert len(id_counter[id_counter < sequence_length].index) == 0\n","\n","            for count in id_counter:\n","                cur_id_length = count - sequence_length + 1\n","                self.range_table.append(self.length + cur_id_length)\n","                self.real_idx_table.append(self.real_length + count)\n","                self.length += cur_id_length\n","                self.real_length += count\n","\n","            dat[\"ID\"] = dat[\"ID\"].astype(\"str\") + f\"_{i}\"\n","            df_list.append(dat.copy())\n","\n","        self.dat = pd.concat(df_list, ignore_index=True)\n","\n","        # 정답 frame 담은 dict 만들기\n","        self.frame_label = dd(lambda: dd(lambda: [-1, -1]))\n","\n","        folder_list = os.listdir(label_root)\n","\n","        for folder in folder_list:\n","            json_list = os.listdir(label_root + \"/\" + folder)\n","\n","            for js in json_list:\n","                with open(label_root + \"/\" + folder + \"/\" + js, \"r\") as j:\n","                    json_dict = json.load(j)\n","\n","                for dict in json_dict[\"annotations\"][\"track\"]:\n","                    if dict[\"@label\"].endswith(\"_start\"):\n","                        cur_id = dict[\"@id\"]\n","                        self.frame_label[js[:-5]][cur_id][0] = dict[\"box\"][0][\"@frame\"]\n","                    elif dict[\"@label\"].endswith(\"_end\"):\n","                        cur_id = dict[\"@id\"]\n","                        self.frame_label[js[:-5]][cur_id][1] = dict[\"box\"][0][\"@frame\"]\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        real_idx = self.find_real_idx(idx)\n","\n","        sequence = self.dat[real_idx : real_idx + self.sequence_length].copy()\n","        target_frames = sequence[\"Frame\"].values\n","        target_filename = sequence[\"Filename\"].unique()[0].split(\".\")[0]\n","        sequence.drop(columns=[\"ID\"], inplace=True)\n","        sequence.drop(columns=[\"Frame\"], inplace=True)\n","        sequence.drop(columns=[\"Filename\"], inplace=True)\n","        # sequence = self.scaler.fit_transform(sequence.values)\n","        sequence = np.array(sequence)\n","\n","        target_labels = []\n","\n","        for target_frame in target_frames:\n","            temp = 0\n","            for cur_id in range(0, len(self.frame_label[target_filename].keys()), 2):\n","                if int(target_frame) >= int(\n","                    self.frame_label[target_filename][str(int(cur_id))][0]\n","                ) and int(target_frame) <= int(\n","                    self.frame_label[target_filename][str(int(cur_id) + 1)][1]\n","                ):\n","                    temp = 1\n","\n","            target_labels.append(temp)\n","\n","        target_labels = torch.LongTensor(target_labels)\n","\n","        return (sequence, target_labels)\n","\n","    def find_real_idx(self, idx):\n","\n","        start = 0\n","        end = len(self.range_table) - 1\n","        while start <= end:\n","            mid = (start + end) // 2\n","            if self.range_table[mid] == idx:\n","                real_idx = idx + ((mid + 1) * (self.sequence_length - 1))\n","                return real_idx\n","\n","            if self.range_table[mid] > idx:\n","                end = mid - 1\n","            else:\n","                start = mid + 1\n","\n","        real_idx = idx + (start * (self.sequence_length - 1))\n","\n","        return real_idx"],"metadata":{"id":"KiMnjyKeHxkN","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"error","timestamp":1716924529492,"user_tz":-540,"elapsed":33,"user":{"displayName":"신유원","userId":"18136435380045022017"}},"outputId":"d402e59b-58f7-4554-9064-88554d8b7e55"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-99ccc3e679db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNormalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"]}]},{"cell_type":"code","source":["import os\n","import os.path as osp\n","import random\n","from argparse import ArgumentParser\n","from datetime import datetime\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"33e9qyDGHgUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dataset import NormalDataset\n","\n","from lstm_ae_old import LSTMAutoencoder\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader, random_split\n","from tqdm import tqdm\n","\n","\n","def parse_args():\n","    parser = ArgumentParser()\n","\n","    # 학습 데이터 경로\n","    parser.add_argument(\n","        \"--root_dir\",\n","        type=str,\n","        default=os.environ.get(\n","            \"SM_CHANNEL_TRAIN_CSV\",\n","            \"/data/ephemeral/home/level2-3-cv-finalproject-cv-06/app/models/lstmae/dataset/normal\",\n","        ),\n","    )\n","\n","    # pth 파일 저장 경로\n","    parser.add_argument(\n","        \"--model_dir\",\n","        type=str,\n","        default=os.environ.get(\n","            \"SM_MODEL_DIR\",\n","            \"/data/ephemeral/home/level2-3-cv-finalproject-cv-06/app/models/pts\",\n","        ),\n","    )\n","\n","    # import_module로 불러올 model name\n","    parser.add_argument(\"--model_name\", type=str, default=\"LSTM\")\n","    # resume 파일 이름\n","    parser.add_argument(\"--resume_name\", type=str, default=\"\")\n","    # random seed\n","    parser.add_argument(\"--seed\", type=int, default=666)\n","\n","    parser.add_argument(\n","        \"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    )\n","    parser.add_argument(\"--num_workers\", type=int, default=8)\n","    parser.add_argument(\"--batch_size\", type=int, default=64)\n","    parser.add_argument(\"--val_batch_size\", type=int, default=64)\n","    parser.add_argument(\"--val_num_workers\", type=int, default=8)\n","    parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n","    parser.add_argument(\"--max_epoch\", type=int, default=50)\n","\n","    parser.add_argument(\"--save_interval\", type=int, default=1)\n","    parser.add_argument(\"--val_interval\", type=int, default=1)\n","    parser.add_argument(\"--thr\", type=float, default=0.02)\n","\n","    parser.add_argument(\"--patience\", type=int, default=10)\n","\n","    parser.add_argument(\"--wandb_mode\", type=str, default=\"online\")\n","    parser.add_argument(\"--wandb_run_name\", type=str, default=\"LSTM-AE\")\n","\n","    args = parser.parse_args()\n","\n","    return args\n","\n","\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","def train(\n","    root_dir,\n","    model_dir,\n","    model_name,\n","    device,\n","    num_workers,\n","    batch_size,\n","    val_num_workers,\n","    val_batch_size,\n","    learning_rate,\n","    max_epoch,\n","    val_interval,\n","    save_interval,\n","    thr,\n","    patience,\n","    resume_name,\n","    seed,\n","    wandb_mode,\n","    wandb_run_name,\n","):\n","\n","    time_start = datetime.now()\n","\n","    train_start = time_start.strftime(\"%Y%m%d_%H%M%S\")\n","\n","    set_seed(seed)\n","\n","    if not osp.exists(model_dir):\n","        os.makedirs(model_dir)\n","\n","    # Define parameters\n","    sequence_length = 20\n","    prediction_time = 1\n","    n_features = 38\n","\n","    batch_size = batch_size\n","    val_batch_size = val_batch_size\n","\n","    # -- early stopping flag\n","    patience = patience\n","    counter = 0\n","\n","    # 데이터셋\n","    dataset = NormalDataset(\n","        root=root_dir,\n","    )\n","\n","    valid_data_size = len(dataset) // 10\n","\n","    train_data_size = len(dataset) - valid_data_size\n","\n","    train_dataset, valid_dataset = random_split(\n","        dataset, lengths=[train_data_size, valid_data_size]\n","    )\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","    )\n","\n","    valid_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=val_batch_size,\n","        shuffle=False,\n","        num_workers=val_num_workers,\n","    )\n","\n","    data_load_end = datetime.now()\n","    data_load_time = data_load_end - time_start\n","    data_load_time = str(data_load_time).split(\".\")[0]\n","    print(f\"==>> data_load_time: {data_load_time}\")\n","\n","    # Initialize the LSTM autoencoder model\n","    # model = LSTMAutoencoder(sequence_length, prediction_time, n_features, 50)\n","    # model.to(device)\n","\n","    model = LSTMAutoEncoder(\n","        num_layers=2, hidden_size=50, n_features=n_features, device=device\n","    )\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(\n","        model.parameters(), lr=learning_rate, weight_decay=1e-6\n","    )\n","\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","        optimizer, milestones=[15, 40], gamma=0.1\n","    )\n","\n","    # if resume_name:\n","    #     optimizer.load_state_dict(load_dict[\"optimizer_state_dict\"])\n","    #     scheduler.load_state_dict(load_dict[\"scheduler_state_dict\"])\n","    #     scaler.load_state_dict(load_dict[\"scaler_state_dict\"])\n","\n","    criterion = nn.MSELoss()\n","    val_criterion = nn.MSELoss(reduction=\"none\")\n","\n","    print(f\"Start training..\")\n","\n","    wandb.init(\n","        project=\"VAD\",\n","        config={\n","            \"lr\": learning_rate,\n","            \"dataset\": \"무인매장\",\n","            \"n_epochs\": max_epoch,\n","            \"loss\": \"MSE\",\n","            \"notes\": \"VAD 실험\",\n","        },\n","        name=wandb_run_name + \"_\" + train_start,\n","        mode=wandb_mode,\n","    )\n","\n","    wandb.watch((model,))\n","\n","    best_loss = np.inf\n","    total_batches = len(train_loader)\n","\n","    for epoch in range(max_epoch):\n","        model.train()\n","\n","        epoch_start = datetime.now()\n","\n","        epoch_loss = 0\n","\n","        for step, data in tqdm(enumerate(train_loader), total=total_batches):\n","\n","            data = data.to(device)\n","            optimizer.zero_grad()\n","\n","            pred = model(data)\n","\n","            loss = criterion(pred, data)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss\n","\n","        epoch_mean_loss = (epoch_loss / total_batches).item()\n","\n","        train_end = datetime.now()\n","        train_time = train_end - epoch_start\n","        train_time = str(train_time).split(\".\")[0]\n","        print(\n","            f\"==>> epoch {epoch+1} train_time: {train_time}\\nloss: {round(epoch_mean_loss,4)}\"\n","        )\n","\n","        if (epoch + 1) % save_interval == 0:\n","\n","            ckpt_fpath = osp.join(model_dir, f\"{model_name}_{train_start}_latest.pth\")\n","\n","            states = {\n","                \"epoch\": epoch,\n","                \"model_name\": model_name,\n","                \"model_state_dict\": model.state_dict(),  # 모델의 state_dict 저장\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict(),\n","                # \"scaler_state_dict\": scaler.state_dict(),\n","            }\n","\n","            torch.save(states, ckpt_fpath)\n","\n","        # validation 주기에 따라 loss를 출력하고 best model을 저장합니다.\n","        if (epoch + 1) % val_interval == 0:\n","\n","            print(f\"Start validation #{epoch+1:2d}\")\n","            model.eval()\n","\n","            with torch.no_grad():\n","                total_loss = 0\n","\n","                for step, data in tqdm(\n","                    enumerate(valid_loader), total=len(valid_loader)\n","                ):\n","\n","                    data = data.to(device)\n","\n","                    pred = model(data)\n","\n","                    val_loss = val_criterion(pred, data)\n","                    val_loss = torch.mean(val_loss)\n","\n","                    total_loss += val_loss\n","\n","                val_mean_loss = (total_loss / len(valid_loader)).item()\n","\n","            if best_loss > val_mean_loss:\n","                print(\n","                    f\"Best performance at epoch: {epoch + 1}, {best_loss:.4f} -> {val_mean_loss:.4f}\"\n","                )\n","                print(f\"Save model in {model_dir}\")\n","                states = {\n","                    \"epoch\": epoch,\n","                    \"model_name\": model_name,\n","                    \"model_state_dict\": model.state_dict(),  # 모델의 state_dict 저장\n","                    # \"optimizer_state_dict\": optimizer.state_dict(),\n","                    # \"scheduler_state_dict\": scheduler.state_dict(),\n","                    # \"scaler_state_dict\": scaler.state_dict(),\n","                    # best.pth는 inference에서만 쓰기?\n","                }\n","\n","                best_ckpt_fpath = osp.join(\n","                    model_dir, f\"{model_name}_{train_start}_best.pth\"\n","                )\n","                torch.save(states, best_ckpt_fpath)\n","                best_loss = val_mean_loss\n","                counter = 0\n","            else:\n","                counter += 1\n","\n","        new_wandb_metric_dict = {\n","            \"train_loss\": epoch_mean_loss,\n","            \"valid_loss\": val_mean_loss,\n","            \"learning_rate\": scheduler.get_lr()[0],\n","        }\n","\n","        wandb.log(new_wandb_metric_dict)\n","\n","        scheduler.step()\n","\n","        epoch_end = datetime.now()\n","        epoch_time = epoch_end - epoch_start\n","        epoch_time = str(epoch_time).split(\".\")[0]\n","        print(\n","            f\"==>> epoch {epoch+1} time: {epoch_time}\\nvalid_loss: {round(val_mean_loss,4)}\"\n","        )\n","\n","        if counter > patience:\n","            print(\"Early Stopping...\")\n","            break\n","\n","    time_end = datetime.now()\n","    total_time = time_end - time_start\n","    total_time = str(total_time).split(\".\")[0]\n","    print(f\"==>> total time: {total_time}\")\n","\n","\n","def main(args):\n","    train(**args.__dict__)\n","\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","\n","    main(args)"],"metadata":{"id":"PLppVeZ0GmWp"},"execution_count":null,"outputs":[]}]}